{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "GlUmtQms9OfS"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchsummary import summary\n",
        "from torch import optim\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torchvision\n",
        "from torchvision import transforms, utils\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "FSIZES = [2,3,4,5,6,7,8]\n",
        "NFILTERS = [2,4,8,16,32]\n",
        "\n",
        "#Pooling layers\n",
        "PSIZES = [2,3,4,5]\n",
        "PTYPE = ['max', 'avg']\n",
        "\n",
        "#Fully connected layers\n",
        "NEURONS = [4,8,16,32,64,128]"
      ],
      "metadata": {
        "id": "fet7DL8E9T7H"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Calcula el tamaño de salida de una capa convolucional'''\n",
        "def conv_out_size(W, K):\n",
        "    return W - K + 3\n",
        "\n",
        "'''Calcula el tamaño de salida de una capa de pooling'''\n",
        "def pool_out_size(W, K):\n",
        "    return math.floor((W - K)/2) + 1"
      ],
      "metadata": {
        "id": "PpJnJjDg9VTc"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoding:\n",
        "    def __init__(self, minC, maxC, minF, maxF):\n",
        "        self.n_conv = random.randint(minC, maxC)\n",
        "        self.n_full = random.randint(minF, maxF)\n",
        "        \n",
        "        \n",
        "        '''First level encoding'''\n",
        "        self.first_level = []\n",
        "        \n",
        "        #Feature extraction part\n",
        "        for i in range(self.n_conv):\n",
        "            layer = {'type' : 'conv',\n",
        "                     'nfilters' : random.choice(NFILTERS),\n",
        "                     'fsize' : random.choice(FSIZES),\n",
        "                     'pool' : random.choice(['max', 'avg', 'off']),\n",
        "                     'psize' : random.choice(PSIZES)\n",
        "                    }\n",
        "            self.first_level.append(layer)\n",
        "        \n",
        "        #Fully connected part\n",
        "        for i in range(self.n_full):\n",
        "            layer = {'type' : 'fc',\n",
        "                     'neurons' : random.choice(NEURONS)}\n",
        "            \n",
        "            self.first_level.append(layer)\n",
        "        \n",
        "        \n",
        "        '''Second level encoding'''\n",
        "        self.second_level = []\n",
        "        prev = -1\n",
        "        for i in range(self.n_conv):\n",
        "            if prev < 1:\n",
        "                prev += 1\n",
        "            if prev >= 1:\n",
        "                for _ in range(prev-1):\n",
        "                    self.second_level.append(random.choice([0,1]))\n",
        "                prev += 1"
      ],
      "metadata": {
        "id": "eYs1pkik9Yw4"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Generando el extractor de caracteristicas (features), el clasificador (classifier), y los tamaños de cada salida (o_sizes)'''\n",
        "def decoding(encoding):\n",
        "    n_conv = encoding.n_conv\n",
        "    n_full = encoding.n_full\n",
        "    first_level = encoding.first_level\n",
        "    second_level = encoding.second_level\n",
        "\n",
        "    features = []\n",
        "    classifier = []\n",
        "    in_channels = 1\n",
        "    out_size = 256\n",
        "    prev = -1\n",
        "    pos = 0\n",
        "    o_sizes = []\n",
        "    for i in range(n_conv):\n",
        "        layer = first_level[i]\n",
        "        n_filters = layer['nfilters']\n",
        "        f_size = layer['fsize']\n",
        "        pad = 1\n",
        "        if f_size > out_size:\n",
        "            f_size = out_size - 1\n",
        "        if i == 0 or i == 1:\n",
        "            if layer['pool'] == 'off':\n",
        "                operation = [\n",
        "                    nn.Conv2d(in_channels=in_channels, out_channels=n_filters, kernel_size=f_size, padding=pad),\n",
        "                    nn.BatchNorm2d(n_filters),\n",
        "                    nn.ReLU(inplace=True)]\n",
        "                in_channels = n_filters\n",
        "                out_size = conv_out_size(out_size, f_size)\n",
        "                o_sizes.append([out_size, in_channels])\n",
        "\n",
        "            if layer['pool'] == 'avg':\n",
        "                p_size = layer['psize']\n",
        "                if p_size > out_size:\n",
        "                    p_size = out_size - 1\n",
        "                operation = [\n",
        "                    nn.Conv2d(in_channels=in_channels, out_channels=n_filters, kernel_size=f_size, padding=pad),\n",
        "                    nn.BatchNorm2d(n_filters),\n",
        "                    nn.ReLU(inplace=True),\n",
        "                    nn.AvgPool2d(kernel_size=p_size, stride=2)]\n",
        "                in_channels = n_filters\n",
        "                out_size = conv_out_size(out_size, f_size)\n",
        "                out_size = pool_out_size(out_size, p_size)\n",
        "                o_sizes.append([out_size, in_channels])\n",
        "\n",
        "            if layer['pool'] == 'max':\n",
        "                p_size = layer['psize']\n",
        "                if p_size > out_size:\n",
        "                    p_size = out_size - 1\n",
        "                operation = [\n",
        "                    nn.Conv2d(in_channels=in_channels, out_channels=n_filters, kernel_size=f_size, padding=pad),\n",
        "                    nn.BatchNorm2d(n_filters),\n",
        "                    nn.ReLU(inplace=True),\n",
        "                    nn.MaxPool2d(kernel_size=p_size, stride=2)]\n",
        "                in_channels = n_filters\n",
        "                out_size = conv_out_size(out_size, f_size)\n",
        "                out_size = pool_out_size(out_size, p_size)\n",
        "                o_sizes.append([out_size, in_channels])\n",
        "        else:\n",
        "            connections = second_level[pos:pos + prev]\n",
        "            for c in range(len(connections)):\n",
        "                if connections[c] == 1:\n",
        "                    in_channels += o_sizes[c][1]\n",
        "\n",
        "            if layer['pool'] == 'off':\n",
        "                operation = [\n",
        "                    nn.Conv2d(in_channels=in_channels, out_channels=n_filters, kernel_size=f_size, padding=pad),\n",
        "                    nn.BatchNorm2d(n_filters),\n",
        "                    nn.ReLU(inplace=True)]\n",
        "                in_channels = n_filters\n",
        "                out_size = conv_out_size(out_size, f_size)\n",
        "                o_sizes.append([out_size, in_channels])\n",
        "\n",
        "            if layer['pool'] == 'avg':\n",
        "                p_size = layer['psize']\n",
        "                if p_size > out_size:\n",
        "                    p_size = out_size - 1\n",
        "                operation = [\n",
        "                    nn.Conv2d(in_channels=in_channels, out_channels=n_filters, kernel_size=f_size, padding=pad),\n",
        "                    nn.BatchNorm2d(n_filters),\n",
        "                    nn.ReLU(inplace=True),\n",
        "                    nn.AvgPool2d(kernel_size=p_size, stride=2)]\n",
        "                in_channels = n_filters\n",
        "                out_size = conv_out_size(out_size, f_size)\n",
        "                out_size = pool_out_size(out_size, p_size)\n",
        "                o_sizes.append([out_size, in_channels])\n",
        "\n",
        "            if layer['pool'] == 'max':\n",
        "                p_size = layer['psize']\n",
        "                if p_size > out_size:\n",
        "                    p_size = out_size - 1\n",
        "                operation = [\n",
        "                    nn.Conv2d(in_channels=in_channels, out_channels=n_filters, kernel_size=f_size, padding=pad),\n",
        "                    nn.BatchNorm2d(n_filters),\n",
        "                    nn.ReLU(inplace=True),\n",
        "                    nn.MaxPool2d(kernel_size=p_size, stride=2)]\n",
        "                in_channels = n_filters\n",
        "                out_size = conv_out_size(out_size, f_size)\n",
        "                out_size = pool_out_size(out_size, p_size)\n",
        "                o_sizes.append([out_size, in_channels])\n",
        "\n",
        "            pos += prev\n",
        "        prev += 1\n",
        "\n",
        "        features.append(operation)\n",
        "    in_size = out_size * out_size * in_channels\n",
        "    for i in range(n_conv, (n_conv + n_full)):\n",
        "        layer = first_level[i]\n",
        "        n_neurons = layer['neurons']\n",
        "        classifier += [nn.Linear(in_size, n_neurons)]\n",
        "        classifier += [nn.ReLU(inplace=True)]\n",
        "        in_size = n_neurons\n",
        "\n",
        "    ##Last layer generates the last neurons for softmax (change this for binary classification)\n",
        "    classifier += [nn.Linear(n_neurons, 3)]\n",
        "\n",
        "    return features, classifier, o_sizes"
      ],
      "metadata": {
        "id": "lJC43HbJ9eWe"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self, encoding, features, classifier, sizes, init_weights=True):\n",
        "        super(CNN, self).__init__()\n",
        "        extraction = []\n",
        "        for layer in features:\n",
        "            extraction += layer\n",
        "        self.extraction = nn.Sequential(*extraction)\n",
        "        self.classifier = nn.Sequential(*classifier)\n",
        "        self.features = features\n",
        "        self.second_level = encoding.second_level\n",
        "        self.sizes = sizes\n",
        "\n",
        "    def forward(self, x):\n",
        "        '''Feature extraction'''\n",
        "        prev = -1\n",
        "        pos = 0\n",
        "        outputs = {}\n",
        "        features = self.features\n",
        "        for i in range(len(features)):\n",
        "            # print('Layer: ', i)\n",
        "            if i == 0 or i == 1:\n",
        "                x = nn.Sequential(*features[i])(x)\n",
        "                outputs[i] = x\n",
        "\n",
        "            else:\n",
        "                connections = self.second_level[pos:pos + prev]\n",
        "                for c in range(len(connections)):\n",
        "                    if connections[c] == 1:\n",
        "                        skip_size = self.sizes[c][0]  # Size comming from previous layer\n",
        "                        req_size = x.shape[2]  # Current feature map size\n",
        "                        if skip_size > req_size:\n",
        "                            psize = skip_size - req_size + 1\n",
        "                            pool = nn.MaxPool2d(kernel_size=psize, stride=1)  # Applying pooling to adjust sizes\n",
        "                            x2 = pool(outputs[c])\n",
        "                        if skip_size == req_size:\n",
        "                            x2 = outputs[c]\n",
        "                        if req_size == skip_size + 1:\n",
        "                            pool = nn.MaxPool2d(kernel_size=2, stride=1, padding=(1, 1))\n",
        "                            x2 = pool(outputs[c])\n",
        "                        if req_size == skip_size + 2:\n",
        "                            pad = int((req_size - skip_size) / 2)\n",
        "                            padding = nn.ZeroPad2d(pad)\n",
        "                            x2 = padding(outputs[c])\n",
        "                        x = torch.cat((x, x2), axis=1)\n",
        "\n",
        "                x = nn.Sequential(*features[i])(x)\n",
        "                outputs[i] = x\n",
        "                pos += prev\n",
        "\n",
        "            prev += 1\n",
        "\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.classifier(x)\n",
        "        return nn.functional.log_softmax(x, dim=1)"
      ],
      "metadata": {
        "id": "F6KaG_eT-Fjm"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'Los bloques convolucionales tienen la forma: (no. de filtros, tamaño del filtro, tipo de pooling, tamaño de pooling)'\n",
        "'Los bloques fully-connected tienen la forma: (no. de neuronas)'\n",
        "e = Encoding(4,4,2,2) #Crear un encoding aleatorio con el número de capas convolucionasl (5 en este ejemplo, repetido dos veces), y de capas fully-connected (2 en el ejemplo, repetido dos veces)\n",
        "#A continuacion cargamos manualmente los datos de la red que hay en excel\n",
        "e.first_level = [{'fsize': 2, 'nfilters': 5, 'pool': 'avg', 'psize': 3, 'type': 'conv'},\n",
        "                {'fsize': 8, 'nfilters': 2, 'pool': 'max', 'psize': 4, 'type': 'conv'},\n",
        "                {'fsize': 4, 'nfilters': 6, 'pool': 'off', 'psize': 4, 'type': 'conv'},\n",
        "                {'fsize': 4, 'nfilters': 4, 'pool': 'off', 'psize': 5, 'type': 'conv'},\n",
        "                {'neurons': 8, 'type': 'fc'},\n",
        "                {'neurons': 8, 'type': 'fc'}]\n",
        "e.second_level = [0,1,0]"
      ],
      "metadata": {
        "id": "Aw-X7Ny3-M0q"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Generamos la decodificacion de la red (genera las partes de la CNN por nosotros de forma automatica)'''\n",
        "cnn = decoding(e)\n",
        "'''Construimos la CNN'''\n",
        "model = CNN(e, cnn[0], cnn[1], cnn[2]).cuda()"
      ],
      "metadata": {
        "id": "MHcqZZN0_08S"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Mounting Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "zCmVSStRAOg4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b619a37c-ae7c-48d7-ca01-077ce8295b63"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "c_labels = np.ones(250, dtype = np.int8) #benigno\n",
        "n_labels = np.zeros(250, dtype = np.int8) #cancer_mama\n",
        "p_labels = np.zeros(250, dtype = np.int8) #saludable\n",
        "#Images path\n",
        "c_root = '/content/gdrive/My Drive/ImagesADP2/benigno/benignop'\n",
        "n_root = '/content/gdrive/My Drive/ImagesADP2/cama/camap'\n",
        "p_root = '/content/gdrive/My Drive/ImagesADP2/salud/saludp'"
      ],
      "metadata": {
        "id": "RlJwqRkBAfju"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CovidDataset(Dataset):\n",
        "  def __init__(self, root, labels, transform = None):\n",
        "    self.root = root #The folder path\n",
        "    self.labels = labels #Labels array\n",
        "    self.transform = transform #Transform composition\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.labels)\n",
        "  \n",
        "  def __getitem__(self, idx):\n",
        "    if torch.is_tensor(idx):\n",
        "      idx = idx.tolist()\n",
        "    p_root = self.root[:] \n",
        "    img_name_p = p_root + str(idx+1) + '.jpg'\n",
        "    #image_p = cv2.imread(img_name_p, 0)\n",
        "    image_p = np.array(Image.open(img_name_p))\n",
        "    [H, W] = image_p.shape\n",
        "    image_p = image_p.reshape((H,W,1))\n",
        "    p_label = self.labels[idx]\n",
        "    sample = {'image': image_p, 'label': p_label}\n",
        "\n",
        "    if self.transform:\n",
        "      sample = self.transform(sample)\n",
        "\n",
        "    return sample\n",
        "\n",
        "#Class to transform image to tensor\n",
        "class ToTensor(object):\n",
        "  def __call__(self, sample):\n",
        "    image, label = sample['image'], sample['label']\n",
        "    image = image.transpose((2,0,1))\n",
        "    return {'image':torch.from_numpy(image),\n",
        "            'label':label}\n",
        "\n",
        "def loading_data():\n",
        "    #Loading Datasets\n",
        "    covid_ds = CovidDataset(root = c_root, labels = c_labels, transform = transforms.Compose([ToTensor()]))\n",
        "    normal_ds = CovidDataset(root = n_root, labels = n_labels, transform = transforms.Compose([ToTensor()]))\n",
        "    pneumonia_ds = CovidDataset(root = p_root, labels = p_labels, transform = transforms.Compose([ToTensor()]))\n",
        "    \n",
        "    #Merging Covid, normal, and pneumonia Datasets\n",
        "    dataset = torch.utils.data.ConcatDataset([covid_ds, normal_ds, pneumonia_ds])\n",
        "    #lengths = [int(len(dataset)*0.7), int(len(dataset)*0.3)+1]\n",
        "    lengths = [int(len(dataset)*0.7), len(dataset)-int(len(dataset)*0.7)]\n",
        "    train_ds, test_ds = torch.utils.data.random_split(dataset = dataset, lengths = lengths)\n",
        "    \n",
        "    #Creating Dataloaders\n",
        "    train_dl = DataLoader(train_ds, batch_size = 24, shuffle = True)\n",
        "    test_dl = DataLoader(test_ds, batch_size = 24, shuffle = True)\n",
        "    \n",
        "    return train_dl, test_dl"
      ],
      "metadata": {
        "id": "SNgxzzaojhMr"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ejemplo de loops de entrenamiento y prueba**"
      ],
      "metadata": {
        "id": "lMO3epip5m-R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Funcion para calcular accuracy\n",
        "def accuracy(output, target, topk=(1,)):\n",
        "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
        "    with torch.no_grad():\n",
        "        maxk = max(topk)\n",
        "        batch_size = target.size(0)\n",
        "\n",
        "        _, pred = output.topk(maxk, 1, True, True)\n",
        "        pred = pred.t()\n",
        "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
        "\n",
        "        res = []\n",
        "        for k in topk:\n",
        "            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
        "            res.append(correct_k.mul_(100.0 / batch_size))\n",
        "        return res"
      ],
      "metadata": {
        "id": "nsy1zcPw5mR-"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "  device1 = torch.device(\"cuda:0\")\n",
        "print(device1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FWshm9yKlzJC",
        "outputId": "4d87c681-cf54-4a52-95ed-92ef4518a080"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''Ejemplo de loop de entrenamiento'''\n",
        "'''Asumimos que los dataloaders de entrenamiento y prueba ya estan disponibles'''\n",
        "train_loader, test_loader=loading_data()\n",
        "\n",
        "epochs = 100\n",
        "lr = 1e-3\n",
        "opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "model.train()\n",
        "acc_sum = 0.0\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  acc_sum=0.0\n",
        "  for i, data in enumerate(train_loader):\n",
        "    x = data['image'].float().cuda()\n",
        "    y = data['label'].long().cuda()\n",
        "  \n",
        "\n",
        "    #Forward pass\n",
        "    pred = model(x)\n",
        "    loss = criterion(pred, y)\n",
        "\n",
        "    #Compute accuracy for a single batch\n",
        "    \n",
        "    acc_sum += accuracy(pred, y)[0]\n",
        "\n",
        "    #Backward pass\n",
        "    loss.backward()\n",
        "\n",
        "    #Optimization step\n",
        "    opt.step()\n",
        "    opt.zero_grad(set_to_none=True)\n",
        "  \n",
        "  #Compute accuracy for this epoch\n",
        "  acc_train = acc_sum/len(train_loader)\n",
        "  print('Accuracy en epoch ', epoch, ' : ', acc_train)"
      ],
      "metadata": {
        "id": "u26a0g8y31X1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Ejemplo de loop de prueba'''\n",
        "'''Asumimos que los dataloaders de entrenamiento y prueba ya estan disponibles'''\n",
        "model.eval()\n",
        "acc_sum = 0.0\n",
        "for i, data in enumerate(test_loader):\n",
        "    x = data['image'].cuda()\n",
        "    y = data['label'].cuda()\n",
        "\n",
        "    #Forward pass\n",
        "    pred = model(x)\n",
        "\n",
        "    #Compute accuracy for a single batch\n",
        "    acc_sum += accuracy(pred, y)\n",
        "\n",
        "#Computing mean accuracy acrros batches\n",
        "acc_total = acc_sum/len(test_loader)\n",
        "print('Accuracy final de prueba: ', acc_total)"
      ],
      "metadata": {
        "id": "l_Nyk8U54yYn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}